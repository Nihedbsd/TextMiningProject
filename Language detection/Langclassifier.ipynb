{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# set the font size of plots\n",
    "plt.rcParams['font.size'] = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_files = ['./langid_data_TUN-AR.txt', './langid_data_ARA.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(filename):\n",
    "    print('Reading file ' + filename + \"...\")\n",
    "    with open(filename, \"r\", encoding='utf8') as textfile:\n",
    "        L = []\n",
    "        for line in textfile:\n",
    "            L.append(line.strip())\n",
    "        print('File contains ', len(L), \"lines.\\n\")\n",
    "        return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file ./langid_data_TUN-AR.txt...\n",
      "File contains  13932 lines.\n",
      "\n",
      "Reading file ./langid_data_ARA.txt...\n",
      "File contains  21787 lines.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tun_corpus = read_text_file(corpus_files[0])\n",
    "ara_corpus = read_text_file(corpus_files[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning\n",
    "To clean our corpus we opted to:\n",
    "\n",
    "- Remove non-word symbols (punctuation, math symbols, emoticons, URLs, hashtags, etc.).\n",
    "- Remove diactritic\n",
    "- Remove documents that contain a large fraction of latin characters (some documents contain english or french words).\n",
    "- Remove very short documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'اهلا بالعالم في هذه التجربه  علامات الترقيم   لا اتذكرها اكثر'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import unicodedata as ud\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "stop_words_ar = stopwords.words('arabic')\n",
    "\n",
    "# regexp for word elongation: matches 3 or more repetitions of a word character.\n",
    "two_plus_letters_RE = re.compile(r\"(\\w)\\1{1,}\", re.DOTALL)\n",
    "three_plus_letters_RE = re.compile(r\"(\\w)\\1{2,}\", re.DOTALL)\n",
    "\n",
    "# regexp for repeated words\n",
    "two_plus_words_RE = re.compile(r\"(\\w+\\s+)\\1{1,}\", re.DOTALL)\n",
    "#arabic ponctuation\n",
    "arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\n",
    "english_punctuations = string.punctuation\n",
    "punctuations_list = arabic_punctuations + english_punctuations\n",
    "\n",
    "def cleanup_text(text):\n",
    "   \n",
    "    # Remove URLs\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', '', text)\n",
    "\n",
    "    # Remove user mentions of the form @username\n",
    "    text = re.sub('@[^\\s]+', '', text)\n",
    "    \n",
    "    # Remove special useless characters such as _x000D_\n",
    "    text = re.sub(r'_[xX]000[dD]_', '', text)\n",
    "\n",
    "    # Remove redundant white spaces\n",
    "    text = text.strip()\n",
    "    text = re.sub('[\\s]+', ' ', text)\n",
    "\n",
    "    # normalize word elongations (characters repeated more than twice)\n",
    "    text = two_plus_letters_RE.sub(r\"\\1\\1\", text)\n",
    "\n",
    "    # remove repeated words\n",
    "    text = two_plus_words_RE.sub(r\"\\1\", text)\n",
    "\n",
    "    #remove repeating char\n",
    "    re.sub(r'(.)\\1+', r'\\1', text)\n",
    "    \n",
    "    #remove diactritic\n",
    "    arabic_diacritics = re.compile(\"\"\"\n",
    "                             ّ    | # Tashdid\n",
    "                             َ    | # Fatha\n",
    "                             ً    | # Tanwin Fath\n",
    "                             ُ    | # Damma\n",
    "                             ٌ    | # Tanwin Damm\n",
    "                             ِ    | # Kasra\n",
    "                             ٍ    | # Tanwin Kasr\n",
    "                             ْ    | # Sukun\n",
    "                             ـ     # Tatwil/Kashida\n",
    "                             \"\"\", re.VERBOSE)\n",
    "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
    "    text = re.sub(\"ى\", \"ي\", text)\n",
    "    text = re.sub(\"ؤ\", \"ء\", text)\n",
    "    text = re.sub(\"ئ\", \"ء\", text)\n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    text = re.sub(\"گ\", \"ك\", text)\n",
    "    text = re.sub(arabic_diacritics, '', text)\n",
    "    \n",
    "    #remove ponctuation\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    text=text.translate(translator)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# unit test of this function\n",
    "cleanup_text(\"أهلاً بالعالم في هذه التجربة ! علامات ،الترقيم ؟ ,? لا .اتذكرها  أكثر\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13932, 21787, 13932, 21787)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clean the corpus using the cleanup_text function \n",
    "\n",
    "tun_corpus_clean = [cleanup_text(document) for document in tun_corpus]\n",
    "ara_corpus_clean = [cleanup_text(document) for document in ara_corpus]\n",
    "len(tun_corpus),len(ara_corpus),len(tun_corpus_clean),len(ara_corpus_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove documents that contain a large fraction of latin characters\n",
    "tun_corpus_clean_2 = []\n",
    "for document in tun_corpus_clean:\n",
    "    if(len(document) > 10 ):\n",
    "        if(len(re.findall('[a-zA-Z]',document))/len(document) < 0.3 ):\n",
    "            tun_corpus_clean_2.append(document)\n",
    "            \n",
    "ara_corpus_clean_2 = []\n",
    "for document in ara_corpus_clean:\n",
    "    if(len(document) > 10 ):\n",
    "        if(len(re.findall('[a-zA-Z]',document))/len(document) < 0.3 ):\n",
    "            ara_corpus_clean_2.append(document) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove very short documents.\n",
    "tun_corpus_clean_3 = []\n",
    "for document in tun_corpus_clean_2:\n",
    "    if(len(document) > 10 ):\n",
    "        tun_corpus_clean_3.append(document)\n",
    "        \n",
    "ara_corpus_clean_3 = []\n",
    "for document in ara_corpus_clean_2:\n",
    "    if(len(document) > 10):\n",
    "        ara_corpus_clean_3.append(document) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a dictionary-based language classifier\n",
    "- **Step 1**: Divide each corpus into a training corpus (70%) and test corpus (30%).\n",
    "- **Step 2**: learn a set of typical words (also called stop words) of **every language** (TUN and ARA) based on its training corpus.\n",
    "- **Step 3**: create a language identification algorithm that takes the list of typical words of each language and a new document as input; and returns the language of this document as output.\n",
    "- **Step 4**: Evaluate the performance of this algorithm based on the test corpus -- calculate classification accuracy, precision, recall, F1, and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide each corpus into a training corpus and test corpus. \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#?train_test_split\n",
    "\n",
    "tun_corpus_clean_train, tun_corpus_clean_test = train_test_split(tun_corpus_clean_3, test_size=0.20, random_state=42 )\n",
    "\n",
    "ara_corpus_clean_train, ara_corpus_clean_test = train_test_split( ara_corpus_clean_3, test_size=0.20, random_state=42 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95 775\n"
     ]
    }
   ],
   "source": [
    "# learn a set of typical words (also called stop words) of every language.\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "P1 = 100  ## configuration hyperparameter; you can modify it if you want; see instructions below.\n",
    "P2 = 500\n",
    "\n",
    "## Find typical words of the TUN language\n",
    "\n",
    "# create TfidfVectorizer instance with maxdf = 1.0 so that the most frequent words of the corpus are NOT thrown away\n",
    "\n",
    "bow_model_tun = TfidfVectorizer (max_df = 1.0, min_df = 0.01)\n",
    "bow_model_ara = TfidfVectorizer (max_df = 1.0, min_df = 0.01)\n",
    "# call fit() method with our TUN corpus; this will create the vocabulary of the corpus ...\n",
    "\n",
    "bow_model_tun.fit( tun_corpus_clean_train )\n",
    "bow_model_ara.fit( ara_corpus_clean_train )\n",
    "# select P words from this vocabulary that have the SMALLEST IDF values -- See the source code in TD1 for help ...\n",
    "\n",
    "print(len(bow_model_tun.get_feature_names()),len(bow_model_ara.get_feature_names()))\n",
    "\n",
    "typical_words_tun=pd.DataFrame(dict(Word=bow_model_tun.get_feature_names(),IDF=bow_model_tun.idf_)).sort_values(\"IDF\", inplace=False, ascending = True)['Word'].head(P1)\n",
    "\n",
    "typical_words_ara=pd.DataFrame(dict(Word=bow_model_ara.get_feature_names(),IDF=bow_model_ara.idf_)).sort_values(\"IDF\", inplace=False, ascending = True)['Word'].head(P1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06387400976423384\n",
      "0.02760641439282796\n"
     ]
    }
   ],
   "source": [
    "# Step 3 -- write the algorithm for dictionary-based language identification. \n",
    "#    This algorithm selects the language that has the highest number of typical words in the input document.\n",
    "\n",
    "## COMPLETE THE CODE BELOW\n",
    "n = 5000\n",
    "random_indices = np.random.choice(np.arange(len(tun_corpus_clean_3)), n, replace=False)\n",
    "small_corpus = [tun_corpus_clean[i] for i in random_indices]\n",
    "def dict_langid(typical_words,document):\n",
    "    words=[]\n",
    "    for word in typical_words:\n",
    "        if word in document and not word in words:\n",
    "            words.append(word)\n",
    "    return len(words)/len(typical_words)\n",
    "\n",
    "\n",
    "fraction_tun=0\n",
    "fraction_ara=0\n",
    "\n",
    "# for each document in the test combined test corpus, call dict_langid with typical_words_tun and then with typical_words_tun\n",
    "for document in small_corpus:\n",
    "    fraction_tun+=dict_langid(typical_words_tun,document)\n",
    "    fraction_tun=fraction_tun/2\n",
    "    fraction_ara+=dict_langid(typical_words_ara,document)\n",
    "    fraction_ara=fraction_ara/2\n",
    "    \n",
    "print(fraction_tun)\n",
    "print(fraction_ara)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8706860706860707 0.8911100658513641\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the performance of this algorithm based on the test corpus\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "def predict_language(document):\n",
    "    fraction_tun=dict_langid(typical_words_tun,document)\n",
    "    fraction_ara=dict_langid(typical_words_ara,document)\n",
    "    if fraction_tun > fraction_ara:\n",
    "        return 'TUN'\n",
    "    else:\n",
    "        return 'ARA'\n",
    "\n",
    "predictions_tun=[]\n",
    "y_true1=[]\n",
    "for document in tun_corpus_clean_test:\n",
    "    predictions_tun.append(predict_language(document))\n",
    "for doc in tun_corpus_clean_test:\n",
    "    y_true1.append(\"TUN\")\n",
    "    \n",
    "\n",
    "predictions_ara=[]\n",
    "y_true2=[]\n",
    "for document in ara_corpus_clean_test:\n",
    "    predictions_ara.append(predict_language(document))\n",
    "for doc in ara_corpus_clean_test:\n",
    "    y_true2.append(\"ARA\")\n",
    "print(accuracy_score(y_true1,predictions_tun),accuracy_score(y_true2,predictions_ara))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a language classifier using supervised learning\n",
    "- **Step 0**: Divide each corpus into a training corpus (70%) and test corpus (30%).\n",
    "- **Step 1**: Create a data frame called ``train_df`` that has two columns: 'document' and 'language'. The 'document' column contains the two corpora concatenated together. The values in the '' column should be 'TUN' and 'ARA'.  Repeat the same thing for the ``test_df``.\n",
    "- **Step 2**: Convert the training documents into numeric feature vectors using the BOW-tfidf method with **character ngrams**.\n",
    "- **Step 3**: Create a language classifier using Naive Bayes method (tfidf version).\n",
    "- **Step 4**: Evaluate performance of this classifier based on the test corpus -- calculate classification accuracy, precision, recall, F1, and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#?train_test_split\n",
    "\n",
    "tun_corpus_clean_train, tun_corpus_clean_test = train_test_split(tun_corpus_clean_3, test_size=0.30, random_state=42 )\n",
    "\n",
    "ara_corpus_clean_train, ara_corpus_clean_test = train_test_split(ara_corpus_clean_3, test_size=0.30, random_state=42 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>عبد الحميد الشيب و العيب كرمك الله اه ها الشيخ...</td>\n",
       "      <td>TUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>والله عندك حق</td>\n",
       "      <td>TUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>هزابي بدلو شواي﻿</td>\n",
       "      <td>TUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>برشه مساطه واسقوطيه وحط في بالك رانا نتفرجو عا...</td>\n",
       "      <td>TUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>حجله واش دورها في هاد المسلسل نشوفها نتعقد رهج...</td>\n",
       "      <td>TUN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            document language\n",
       "0  عبد الحميد الشيب و العيب كرمك الله اه ها الشيخ...      TUN\n",
       "1                                      والله عندك حق      TUN\n",
       "2                                   هزابي بدلو شواي﻿      TUN\n",
       "3  برشه مساطه واسقوطيه وحط في بالك رانا نتفرجو عا...      TUN\n",
       "4  حجله واش دورها في هاد المسلسل نشوفها نتعقد رهج...      TUN"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create 2 data frames called train_df and test_df\n",
    "\n",
    "# create data frame\n",
    "train_df = pd.DataFrame({'document':[], 'language':[]})\n",
    "test_df = pd.DataFrame({'document':[], 'language':[]})\n",
    "\n",
    "# fill the language column\n",
    "train_df.language = pd.Series(['TUN']*len(tun_corpus_clean_train) + ['ARA']*len(ara_corpus_clean_train))\n",
    "test_df.language = pd.Series(['TUN']*len(tun_corpus_clean_test) + ['ARA']*len(ara_corpus_clean_test))\n",
    "\n",
    "# fill the document column -- CONCATENATE the TUN CORPUS and ARA CORPUS\n",
    "train_df.document = pd.Series(tun_corpus_clean_train + ara_corpus_clean_train)\n",
    "test_df.document = pd.Series(tun_corpus_clean_test + ara_corpus_clean_test)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the training documents into numeric feature vectors using the BOW-tfidf method with character ngrams.\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "\n",
    "n = 4   # hyperparameter for of character ngrams\n",
    "\n",
    "#Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "#the vector\n",
    "# Create an instance of TfidfVectorizer class with analyzer = 'char' so that it generates bag of characters and not bag of words\n",
    "bow_model_char = TfidfVectorizer(analyzer='char', ngram_range=(1,n), max_df = 0.9, min_df = 0.01)\n",
    "\n",
    "# Call fit method with the combined training corpus\n",
    "bow_model_char.fit(train_df.document)\n",
    "\n",
    "#save the bow model\n",
    "pickle.dump(bow_model_char, open('bow_model.sav', 'wb'))\n",
    "\n",
    "# Create DTM(document term matrix) matrix of the combined training corpus and test corpus\n",
    "train_dtm = bow_model_char.transform(train_df.document)\n",
    "test_dtm = bow_model_char.transform(test_df.document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TUN\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#instance of multinomial nb model\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "nb_model.fit(train_dtm,train_df.language)\n",
    "\n",
    "#unit test of the model\n",
    "test=[]\n",
    "test.append(\"يخي شبيك تحكي هكا\")\n",
    "test1=bow_model_char.transform(test)\n",
    "print(nb_model.predict(test1)[0])\n",
    "y_pred = nb_model.predict(test_dtm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score :  0.9485227841762643\n"
     ]
    }
   ],
   "source": [
    "# Step 4   Use the same source code as in Step 4 of the previous part.\n",
    "print(\"Accuracy score : \",accuracy_score(test_df.language, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "filename = 'lang_model.sav'\n",
    "pickle.dump(nb_model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open('lang_model.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3351527058028016\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'OTHER'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test new instance\n",
    "def predict_lang(text):\n",
    "    inst=[]\n",
    "    inst.append(text)\n",
    "    text_vect=bow_model_char.transform(inst)\n",
    "    prob=loaded_model.predict_proba(text_vect)[0]\n",
    "    x=prob[0]\n",
    "    print(x)\n",
    "    if x <0.8 and x>0.2:\n",
    "        return(\"OTHER\")\n",
    "    else:\n",
    "        return nb_model.predict(text_vect)[0]\n",
    "    \n",
    "predict_lang(\"oaded_model hqjdhkqdjh zajdhazdlkamdk jqdqjdmaze لحميد العيب \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
